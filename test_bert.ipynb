{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "\n",
    "# Load your test dataset\n",
    "test_dataset = load_dataset(\"md_gender_bias\", name=\"convai2_inferred\")[\"test\"]\n",
    "\n",
    "# Replace 'regression_model' with the actual path where you saved your model\n",
    "model_path = 'model_output'\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", max_length=1024)\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    new_dataset_dict = {\n",
    "        'text': [entry['text'] for entry in dataset],\n",
    "        'labels': [entry['binary_score'] if entry['binary_label'] == 1 else 1 - entry['binary_score'] for entry in dataset]\n",
    "    }\n",
    "    new_dataset = Dataset.from_dict(new_dataset_dict)\n",
    "    return new_dataset\n",
    "\n",
    "# Tokenize the test dataset\n",
    "test_dataset = process_dataset(test_dataset)\n",
    "tokenized_test_dataset = test_dataset.map(lambda examples: tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True), batched=True)\n",
    "\n",
    "\n",
    "\n",
    "# Set up Trainer with custom data collator\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# Extract the predicted values\n",
    "predicted_values = predictions.predictions.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: 0.5456048250198364\n"
     ]
    }
   ],
   "source": [
    "# test cell\n",
    "text = \"hello what are doing today ?\"\n",
    "tokenized_input = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**tokenized_input).logits\n",
    "\n",
    "# Convert logits to predicted value\n",
    "predicted_value = logits.squeeze().item()\n",
    "\n",
    "# Print the predicted value\n",
    "print(\"Predicted Value:\", predicted_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
